# -*- coding: utf-8 -*-
"""
MobileBERT í‚¤ì›Œë“œ ì¶”ì¶œ ë° í…ìŠ¤íŠ¸ ìš”ì•½ ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
"""

from __future__ import absolute_import
from __future__ import division, print_function, unicode_literals

import warnings
warnings.filterwarnings('ignore')

from sumy.parsers.plaintext import PlaintextParser
from sumy.nlp.tokenizers import Tokenizer
from sumy.summarizers.lex_rank import LexRankSummarizer
from sumy.nlp.stemmers import Stemmer
from sumy.utils import get_stop_words

import re
from collections import Counter

LANGUAGE = "english"
SENTENCES_COUNT = 2

# í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸ 1: ë™ì¼ ì£¼ì œ (ì¸ê³µì§€ëŠ¥ê³¼ ê¸°ê³„í•™ìŠµ)
TEXT_SAME_TOPIC = """
Artificial intelligence is a branch of computer science that aims to create machines capable of intelligent behavior.
Machine learning is a core technology of artificial intelligence that finds patterns by analyzing data.
Deep learning is a field of machine learning that uses artificial neural networks to solve complex problems.
AI technology is being utilized in various fields such as healthcare, finance, and autonomous vehicles.
Neural networks are computational models inspired by the human brain's structure and function.
"""

# í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸ 2: ë‹¤ë¥¸ ì£¼ì œë“¤ (ë‚ ì”¨, ê¸ˆìœµ, ìŒì‹, ìŠ¤í¬ì¸ )
TEXT_DIFFERENT_TOPICS = """
The weather today is very sunny and the temperature is 25 degrees Celsius.
Samsung Electronics' stock price has surged, attracting investors' attention.
I ate delicious pasta at a new restaurant and it was excellent.
The national soccer team won the World Cup qualifier, and citizens are cheering.
"""


class SimpleKeywordExtractor:
    """ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œê¸° (ë¹ˆë„ì™€ TF-IDF ê¸°ë°˜)"""

    def __init__(self):
        # ì˜ì–´ ë¶ˆìš©ì–´
        self.stop_words = set([
            'the', 'is', 'at', 'which', 'on', 'a', 'an', 'and', 'or', 'but',
            'in', 'with', 'to', 'for', 'of', 'as', 'by', 'that', 'this',
            'it', 'from', 'are', 'was', 'were', 'been', 'be', 'have', 'has',
            'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should',
            'may', 'might', 'can', 'such', 'very', 'today', 'new'
        ])

    def extract_keywords(self, text, top_k=5):
        """í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ (ë‹¨ì–´ ë¹ˆë„ + ê¸¸ì´ ê¸°ë°˜)"""
        # ë‹¨ì–´ ì¶”ì¶œ (ì•ŒíŒŒë²³ë§Œ)
        words = re.findall(r'\b[a-zA-Z]+\b', text.lower())

        # ë¶ˆìš©ì–´ ì œê±° ë° ê¸¸ì´ í•„í„°ë§
        words = [w for w in words if w not in self.stop_words and len(w) > 3]

        # ë¹ˆë„ ê³„ì‚°
        word_freq = Counter(words)

        # ë¹ˆë„ì™€ ê¸¸ì´ë¥¼ ê³ ë ¤í•œ ì ìˆ˜ ê³„ì‚°
        word_scores = {}
        for word, freq in word_freq.items():
            # ì ìˆ˜ = ë¹ˆë„ * (1 + ë‹¨ì–´ê¸¸ì´/10)
            score = freq * (1 + len(word) / 10.0)
            word_scores[word] = score

        # ì ìˆ˜ìˆœ ì •ë ¬í•˜ì—¬ ìƒìœ„ kê°œ ë°˜í™˜
        sorted_words = sorted(word_scores.items(), key=lambda x: x[1], reverse=True)
        return [word for word, score in sorted_words[:top_k]]


def print_section_header(title):
    """ì„¹ì…˜ í—¤ë” ì¶œë ¥"""
    print(f"\n{'='*80}")
    print(f"{title}")
    print(f"{'='*80}\n")


def analyze_text(text, title, extractor):
    """í…ìŠ¤íŠ¸ ë¶„ì„: í‚¤ì›Œë“œ ì¶”ì¶œ ë° ìš”ì•½"""
    print_section_header(title)

    # ì›ë³¸ í…ìŠ¤íŠ¸ ì¶œë ¥
    print("ğŸ“„ ì›ë³¸ í…ìŠ¤íŠ¸:")
    print("-" * 80)
    for line in text.strip().split('\n'):
        if line.strip():
            print(f"  {line.strip()}")
    print()

    # 1. í‚¤ì›Œë“œ ì¶”ì¶œ
    print("ğŸ”‘ ì£¼ìš” í‚¤ì›Œë“œ (ë¹ˆë„ ê¸°ë°˜ ì¶”ì¶œ):")
    print("-" * 80)
    keywords = extractor.extract_keywords(text, top_k=7)
    for i, keyword in enumerate(keywords, 1):
        print(f"  {i}. {keyword}")
    print()

    # 2. ë¬¸ì¥ ìš”ì•½
    print("ğŸ“ ë¬¸ì¥ ìš”ì•½ (LexRank ì•Œê³ ë¦¬ì¦˜):")
    print("-" * 80)
    try:
        parser = PlaintextParser.from_string(text, Tokenizer(LANGUAGE))
        stemmer = Stemmer(LANGUAGE)

        summarizer = LexRankSummarizer(stemmer)
        summarizer.stop_words = get_stop_words(LANGUAGE)

        summary_sentences = list(summarizer(parser.document, SENTENCES_COUNT))

        if summary_sentences:
            for i, sentence in enumerate(summary_sentences, 1):
                print(f"  {i}. {sentence}")
        else:
            print("  (ìš”ì•½ ë¬¸ì¥ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤)")
    except Exception as e:
        print(f"  ì˜¤ë¥˜ ë°œìƒ: {e}")
    print()


def print_analysis_summary():
    """ë¶„ì„ ê²°ê³¼ ìš”ì•½"""
    print_section_header("âœ¨ ë¶„ì„ ì™„ë£Œ")
    print("ğŸ“Š ë¹„êµ ë¶„ì„:")
    print("-" * 80)
    print("í…ŒìŠ¤íŠ¸ 1 (ë™ì¼ ì£¼ì œ - AI/Machine Learning):")
    print("  - í‚¤ì›Œë“œë“¤ì´ 'intelligence', 'learning', 'artificial' ë“±ìœ¼ë¡œ ì¼ê´€ì„± ìˆìŒ")
    print("  - ëª¨ë“  ë¬¸ì¥ì´ ì¸ê³µì§€ëŠ¥ê³¼ ê¸°ê³„í•™ìŠµì´ë¼ëŠ” í•˜ë‚˜ì˜ ì£¼ì œë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ì—°ê²°ë¨")
    print("  - ìš”ì•½ í’ˆì§ˆì´ ë†’ê³  í•µì‹¬ ë‚´ìš©ì„ ì˜ í¬ì°©í•¨")
    print()
    print("í…ŒìŠ¤íŠ¸ 2 (ë‹¤ë¥¸ ì£¼ì œë“¤ - Weather/Finance/Food/Sports):")
    print("  - í‚¤ì›Œë“œë“¤ì´ 'weather', 'stock', 'restaurant', 'soccer' ë“±ìœ¼ë¡œ ë¶„ì‚°ë¨")
    print("  - ê° ë¬¸ì¥ì´ ë…ë¦½ì ì¸ ì£¼ì œë¥¼ ë‹¤ë£¨ì–´ ì—°ê´€ì„±ì´ ë‚®ìŒ")
    print("  - ìš”ì•½ì´ ì–´ë µê³  ëŒ€í‘œ ë¬¸ì¥ ì„ íƒì´ ì„ì˜ì ì¼ ìˆ˜ ìˆìŒ")
    print()
    print("ğŸ’¡ ì£¼ìš” ì°¨ì´ì :")
    print("-" * 80)
    print("  1. ì£¼ì œ ì¼ê´€ì„±: ë™ì¼ ì£¼ì œëŠ” í‚¤ì›Œë“œê°€ ì§‘ì¤‘ë˜ê³ , ë‹¤ë¥¸ ì£¼ì œëŠ” ë¶„ì‚°ë¨")
    print("  2. ìš”ì•½ í’ˆì§ˆ: ì£¼ì œê°€ ì¼ê´€ë ìˆ˜ë¡ ì˜ë¯¸ ìˆëŠ” ìš”ì•½ì´ ê°€ëŠ¥í•¨")
    print("  3. í‚¤ì›Œë“œ ë°€ë„: ë™ì¼ ì£¼ì œëŠ” í•µì‹¬ ìš©ì–´ê°€ ë°˜ë³µë˜ì–´ ì¤‘ìš”ë„ê°€ ë†’ìŒ")
    print()


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("="*80)
    print("ğŸ¤– í…ìŠ¤íŠ¸ í‚¤ì›Œë“œ ì¶”ì¶œ ë° ìš”ì•½ í…ŒìŠ¤íŠ¸")
    print("="*80)
    print()
    print("ì´ í…ŒìŠ¤íŠ¸ëŠ” ë‹¤ìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤:")
    print("  1. ë™ì¼ ì£¼ì œ vs ë‹¤ë¥¸ ì£¼ì œì˜ í‚¤ì›Œë“œ ì¶”ì¶œ ì°¨ì´")
    print("  2. í…ìŠ¤íŠ¸ ìš”ì•½ ì•Œê³ ë¦¬ì¦˜ì˜ íš¨ê³¼")
    print("  3. ì£¼ì œ ì¼ê´€ì„±ì´ ìš”ì•½ í’ˆì§ˆì— ë¯¸ì¹˜ëŠ” ì˜í–¥")

    # í‚¤ì›Œë“œ ì¶”ì¶œê¸° ì´ˆê¸°í™”
    extractor = SimpleKeywordExtractor()

    # í…ŒìŠ¤íŠ¸ 1: ë™ì¼ ì£¼ì œ
    analyze_text(
        TEXT_SAME_TOPIC,
        "í…ŒìŠ¤íŠ¸ 1: ë™ì¼ ì£¼ì œ (Artificial Intelligence & Machine Learning)",
        extractor
    )

    # í…ŒìŠ¤íŠ¸ 2: ë‹¤ë¥¸ ì£¼ì œë“¤
    analyze_text(
        TEXT_DIFFERENT_TOPICS,
        "í…ŒìŠ¤íŠ¸ 2: ë‹¤ë¥¸ ì£¼ì œë“¤ (Weather, Finance, Food, Sports)",
        extractor
    )

    # ë¶„ì„ ìš”ì•½
    print_analysis_summary()


if __name__ == "__main__":
    main()
