# -*- coding: utf-8 -*-
"""
MobileBERT í‚¤ì›Œë“œ ì¶”ì¶œ ë° í…ìŠ¤íŠ¸ ìš”ì•½ í…ŒìŠ¤íŠ¸
ë™ì¼ ì£¼ì œì™€ ë‹¤ë¥¸ ì£¼ì œì˜ í…ìŠ¤íŠ¸ë¥¼ ë¹„êµ ë¶„ì„
"""

from __future__ import absolute_import
from __future__ import division, print_function, unicode_literals

import warnings
warnings.filterwarnings('ignore')

from sumy.parsers.plaintext import PlaintextParser
from sumy.nlp.tokenizers import Tokenizer
from sumy.summarizers.lex_rank import LexRankSummarizer
from sumy.nlp.stemmers import Stemmer
from sumy.utils import get_stop_words

try:
    from transformers import AutoTokenizer, AutoModel
    import torch
    from collections import Counter
    import numpy as np
    MOBILEBERT_AVAILABLE = True
except ImportError:
    MOBILEBERT_AVAILABLE = False
    print("ê²½ê³ : transformers ë˜ëŠ” torchê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    print("ì„¤ì¹˜ ë°©ë²•: pip install transformers torch")


LANGUAGE = "korean"
SENTENCES_COUNT = 2

# í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸ 1: ë™ì¼ ì£¼ì œ (ì¸ê³µì§€ëŠ¥)
TEXT_SAME_TOPIC = """
ì¸ê³µì§€ëŠ¥ì€ ì»´í“¨í„°ê°€ ì¸ê°„ì²˜ëŸ¼ ìƒê°í•˜ê³  í•™ìŠµí•  ìˆ˜ ìˆê²Œ ë§Œë“œëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.
ë¨¸ì‹ ëŸ¬ë‹ì€ ì¸ê³µì§€ëŠ¥ì˜ í•µì‹¬ ê¸°ìˆ ë¡œ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ íŒ¨í„´ì„ ì°¾ì•„ëƒ…ë‹ˆë‹¤.
ë”¥ëŸ¬ë‹ì€ ì¸ê³µ ì‹ ê²½ë§ì„ ì‚¬ìš©í•˜ì—¬ ë³µì¡í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹ì˜ í•œ ë¶„ì•¼ì…ë‹ˆë‹¤.
ì¸ê³µì§€ëŠ¥ ê¸°ìˆ ì€ ì˜ë£Œ, ê¸ˆìœµ, ììœ¨ì£¼í–‰ì°¨ ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì— í™œìš©ë˜ê³  ìˆìŠµë‹ˆë‹¤.
"""

# í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸ 2: ë‹¤ë¥¸ ì£¼ì œë“¤
TEXT_DIFFERENT_TOPICS = """
ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ë§¤ìš° í™”ì°½í•˜ê³  ê¸°ì˜¨ì€ ì„­ì”¨ 25ë„ì…ë‹ˆë‹¤.
ì‚¼ì„±ì „ìì˜ ì£¼ê°€ê°€ ê¸‰ë“±í•˜ë©´ì„œ íˆ¬ììë“¤ì˜ ê´€ì‹¬ì´ ì§‘ì¤‘ë˜ê³  ìˆìŠµë‹ˆë‹¤.
ìƒˆë¡œìš´ ë ˆìŠ¤í† ë‘ì—ì„œ ë§›ìˆëŠ” íŒŒìŠ¤íƒ€ë¥¼ ë¨¹ì—ˆëŠ”ë° ì •ë§ í›Œë¥­í–ˆìŠµë‹ˆë‹¤.
ì¶•êµ¬ êµ­ê°€ëŒ€í‘œíŒ€ì´ ì›”ë“œì»µ ì˜ˆì„ ì—ì„œ ìŠ¹ë¦¬í•˜ì—¬ êµ­ë¯¼ë“¤ì´ í™˜í˜¸í•˜ê³  ìˆìŠµë‹ˆë‹¤.
"""


class MobileBERTKeywordExtractor:
    """MobileBERTë¥¼ ì‚¬ìš©í•œ í‚¤ì›Œë“œ ì¶”ì¶œê¸°"""

    def __init__(self):
        if not MOBILEBERT_AVAILABLE:
            self.model = None
            self.tokenizer = None
            return

        print("MobileBERT ëª¨ë¸ ë¡œë”© ì¤‘...")
        # ë‹¤êµ­ì–´ ì§€ì› MobileBERT ëª¨ë¸ ì‚¬ìš©
        self.model_name = "google/mobilebert-uncased"
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            self.model = AutoModel.from_pretrained(self.model_name)
            self.model.eval()
            print("MobileBERT ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
        except Exception as e:
            print(f"ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            self.model = None
            self.tokenizer = None

    def extract_keywords(self, text, top_k=5):
        """í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        if not MOBILEBERT_AVAILABLE or self.model is None:
            # MobileBERTê°€ ì—†ì„ ê²½ìš° ê°„ë‹¨í•œ ë‹¨ì–´ ë¹ˆë„ ê¸°ë°˜ ì¶”ì¶œ
            return self._extract_keywords_simple(text, top_k)

        # í† í°í™”
        inputs = self.tokenizer(text, return_tensors="pt", truncation=True, max_length=512)

        # ëª¨ë¸ ì¶”ë¡ 
        with torch.no_grad():
            outputs = self.model(**inputs)
            # ë§ˆì§€ë§‰ hidden state ì‚¬ìš©
            hidden_states = outputs.last_hidden_state[0]

        # í† í°ë³„ ì¤‘ìš”ë„ ê³„ì‚° (L2 norm ì‚¬ìš©)
        token_importance = torch.norm(hidden_states, dim=1).numpy()

        # í† í°ê³¼ ì¤‘ìš”ë„ ë§¤í•‘
        tokens = self.tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])

        # íŠ¹ìˆ˜ í† í° ì œì™¸í•˜ê³  ì¤‘ìš”ë„ ê¸°ë°˜ ì •ë ¬
        token_scores = []
        for token, score in zip(tokens, token_importance):
            if token not in ['[CLS]', '[SEP]', '[PAD]'] and not token.startswith('##'):
                token_scores.append((token, float(score)))

        # ì¤‘ìš”ë„ ìˆœìœ¼ë¡œ ì •ë ¬
        token_scores.sort(key=lambda x: x[1], reverse=True)

        # ìƒìœ„ kê°œ í‚¤ì›Œë“œ ë°˜í™˜
        keywords = [token for token, score in token_scores[:top_k]]

        return keywords

    def _extract_keywords_simple(self, text, top_k=5):
        """ê°„ë‹¨í•œ ë‹¨ì–´ ë¹ˆë„ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ (fallback)"""
        # í•œê¸€ë§Œ ì¶”ì¶œ
        import re
        from collections import Counter
        words = re.findall(r'[ê°€-í£]+', text)

        # ë¶ˆìš©ì–´ ì œê±° (ê°„ë‹¨í•œ ë²„ì „)
        stop_words = ['ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì—', 'ì˜', 'ì™€', 'ê³¼', 'ë„', 'ë¡œ', 'ìœ¼ë¡œ', 'ì…ë‹ˆë‹¤', 'ìˆìŠµë‹ˆë‹¤', 'ë©ë‹ˆë‹¤', 'ìˆ˜', 'ë“±']
        words = [w for w in words if w not in stop_words and len(w) > 1]

        # ë¹ˆë„ ê³„ì‚°
        word_freq = Counter(words)

        # ìƒìœ„ kê°œ ë°˜í™˜
        return [word for word, freq in word_freq.most_common(top_k)]


def print_section_header(title):
    """ì„¹ì…˜ í—¤ë” ì¶œë ¥"""
    print(f"\n{'='*80}")
    print(f"{title}")
    print(f"{'='*80}\n")


def analyze_text(text, title, extractor):
    """í…ìŠ¤íŠ¸ ë¶„ì„: í‚¤ì›Œë“œ ì¶”ì¶œ ë° ìš”ì•½"""
    print_section_header(title)

    # ì›ë³¸ í…ìŠ¤íŠ¸ ì¶œë ¥
    print("ğŸ“„ ì›ë³¸ í…ìŠ¤íŠ¸:")
    print("-" * 80)
    print(text.strip())
    print()

    # 1. í‚¤ì›Œë“œ ì¶”ì¶œ
    print("ğŸ”‘ ì£¼ìš” í‚¤ì›Œë“œ (MobileBERT):")
    print("-" * 80)
    keywords = extractor.extract_keywords(text, top_k=5)
    for i, keyword in enumerate(keywords, 1):
        print(f"  {i}. {keyword}")
    print()

    # 2. ë¬¸ì¥ ìš”ì•½
    print("ğŸ“ ë¬¸ì¥ ìš”ì•½ (LexRank ì•Œê³ ë¦¬ì¦˜):")
    print("-" * 80)
    parser = PlaintextParser.from_string(text, Tokenizer(LANGUAGE))
    stemmer = Stemmer(LANGUAGE)

    summarizer = LexRankSummarizer(stemmer)
    summarizer.stop_words = get_stop_words(LANGUAGE)

    summary_sentences = list(summarizer(parser.document, SENTENCES_COUNT))

    if summary_sentences:
        for i, sentence in enumerate(summary_sentences, 1):
            print(f"  {i}. {sentence}")
    else:
        print("  (ìš”ì•½ ë¬¸ì¥ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤)")
    print()


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("="*80)
    print("MobileBERT í‚¤ì›Œë“œ ì¶”ì¶œ ë° í…ìŠ¤íŠ¸ ìš”ì•½ í…ŒìŠ¤íŠ¸")
    print("="*80)

    # MobileBERT í‚¤ì›Œë“œ ì¶”ì¶œê¸° ì´ˆê¸°í™”
    extractor = MobileBERTKeywordExtractor()

    # í…ŒìŠ¤íŠ¸ 1: ë™ì¼ ì£¼ì œ
    analyze_text(
        TEXT_SAME_TOPIC,
        "í…ŒìŠ¤íŠ¸ 1: ë™ì¼ ì£¼ì œ (ì¸ê³µì§€ëŠ¥)",
        extractor
    )

    # í…ŒìŠ¤íŠ¸ 2: ë‹¤ë¥¸ ì£¼ì œë“¤
    analyze_text(
        TEXT_DIFFERENT_TOPICS,
        "í…ŒìŠ¤íŠ¸ 2: ë‹¤ë¥¸ ì£¼ì œë“¤ (ë‚ ì”¨, ì£¼ì‹, ìŒì‹, ìŠ¤í¬ì¸ )",
        extractor
    )

    print_section_header("ë¶„ì„ ì™„ë£Œ!")
    print("ğŸ’¡ ë¹„êµ ë¶„ì„:")
    print("-" * 80)
    print("- í…ŒìŠ¤íŠ¸ 1 (ë™ì¼ ì£¼ì œ): í‚¤ì›Œë“œë“¤ì´ 'ì¸ê³µì§€ëŠ¥' ê´€ë ¨ ìš©ì–´ë¡œ ì¼ê´€ì„± ìˆìŒ")
    print("- í…ŒìŠ¤íŠ¸ 2 (ë‹¤ë¥¸ ì£¼ì œ): í‚¤ì›Œë“œë“¤ì´ ë‹¤ì–‘í•œ ì£¼ì œì— ê±¸ì³ ë¶„ì‚°ë¨")
    print("- ìš”ì•½ í’ˆì§ˆì€ í…ìŠ¤íŠ¸ì˜ ì£¼ì œ ì¼ê´€ì„±ì— ì˜í–¥ì„ ë°›ìŒ")
    print()


if __name__ == "__main__":
    main()
